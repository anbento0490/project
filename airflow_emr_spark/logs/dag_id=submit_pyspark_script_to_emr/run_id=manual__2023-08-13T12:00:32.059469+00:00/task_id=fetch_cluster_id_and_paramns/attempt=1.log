[2023-08-13 12:00:33,362] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: submit_pyspark_script_to_emr.fetch_cluster_id_and_paramns manual__2023-08-13T12:00:32.059469+00:00 [queued]>
[2023-08-13 12:00:33,378] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: submit_pyspark_script_to_emr.fetch_cluster_id_and_paramns manual__2023-08-13T12:00:32.059469+00:00 [queued]>
[2023-08-13 12:00:33,380] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-08-13 12:00:33,381] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-08-13 12:00:33,381] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-08-13 12:00:33,400] {taskinstance.py:1377} INFO - Executing <Task(PythonOperator): fetch_cluster_id_and_paramns> on 2023-08-13 12:00:32.059469+00:00
[2023-08-13 12:00:33,407] {standard_task_runner.py:52} INFO - Started process 176 to run task
[2023-08-13 12:00:33,412] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'submit_pyspark_script_to_emr', 'fetch_cluster_id_and_paramns', 'manual__2023-08-13T12:00:32.059469+00:00', '--job-id', '34', '--raw', '--subdir', 'DAGS_FOLDER/orchestration/submit_spark_job_to_emr.py', '--cfg-path', '/tmp/tmppvkober_', '--error-file', '/tmp/tmpcgmuz9_4']
[2023-08-13 12:00:33,414] {standard_task_runner.py:80} INFO - Job 34: Subtask fetch_cluster_id_and_paramns
[2023-08-13 12:00:33,494] {task_command.py:370} INFO - Running <TaskInstance: submit_pyspark_script_to_emr.fetch_cluster_id_and_paramns manual__2023-08-13T12:00:32.059469+00:00 [running]> on host airflow_scheduler
[2023-08-13 12:00:33,597] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=submit_pyspark_script_to_emr
AIRFLOW_CTX_TASK_ID=fetch_cluster_id_and_paramns
AIRFLOW_CTX_EXECUTION_DATE=2023-08-13T12:00:32.059469+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-08-13T12:00:32.059469+00:00
[2023-08-13 12:00:33,611] {base.py:68} INFO - Using connection ID 'aws_default' for task execution.
[2023-08-13 12:00:33,613] {base_aws.py:210} INFO - Credentials retrieved from extra_config
[2023-08-13 12:00:33,614] {base_aws.py:100} INFO - Retrieving region_name from Connection.extra_config['region_name']
[2023-08-13 12:00:34,223] {emr.py:66} INFO - Found cluster name = finance-emr id = j-1QC6TK3TU9Q02
[2023-08-13 12:00:34,240] {submit_spark_job_to_emr.py:56} INFO - Fetched LOCAL CONFIG is:
 /opt/airflow/dags
 /computation/src/pyspark_scripts/ 
 read_and_write_back_to_s3.py 
 /opt/airflow/dags/computation/src/pyspark_scripts/read_and_write_back_to_s3.py 

[2023-08-13 12:00:34,241] {submit_spark_job_to_emr.py:57} INFO - Fetched S3 CONFIG is:
 emr-data-947775527574
 scripts/read_and_write_back_to_s3.py 
 s3://emr-data-947775527574/src/boa/ 
 s3://emr-data-947775527574/tgt/boa/ 

[2023-08-13 12:00:34,242] {submit_spark_job_to_emr.py:58} INFO - Fetched SPARK S3 CONFIG is:
 ['spark-submit', '--deploy-mode', 'client']
 {'spark.sql.session.timeZone': 'UTC', 'spark.speculation': 'false'} 

[2023-08-13 12:00:34,244] {submit_spark_job_to_emr.py:59} INFO - JAR STRING is:
 s3://emr-data-947775527574/airflow-finance/dags/assets/jars/spark-3.3.1/spark-snowflake_2.9.1/snowflake-ingest-sdk-0.10.3.jar,s3://emr-data-947775527574/airflow-finance/dags/assets/jars/spark-3.3.1/spark-snowflake_2.9.1/snowflake-jdbc-3.13.10.jar,s3://emr-data-947775527574/airflow-finance/dags/assets/jars/spark-3.3.1/spark-snowflake_2.9.1/spark-snowflake_2.12-2.9.3-spark_3.1.jar 

[2023-08-13 12:00:34,247] {python.py:173} INFO - Done. Returned value was: ('j-1QC6TK3TU9Q02', '/opt/airflow/dags', '/computation/src/pyspark_scripts/', 'read_and_write_back_to_s3.py', '/opt/airflow/dags/computation/src/pyspark_scripts/read_and_write_back_to_s3.py', 'emr-data-947775527574', 'scripts/read_and_write_back_to_s3.py', 's3://emr-data-947775527574/tgt/boa/', 's3://emr-data-947775527574/src/boa/', ['spark-submit', '--deploy-mode', 'client'], {'spark.sql.session.timeZone': 'UTC', 'spark.speculation': 'false'}, 's3://emr-data-947775527574/airflow-finance/dags/assets/jars/spark-3.3.1/spark-snowflake_2.9.1/snowflake-ingest-sdk-0.10.3.jar,s3://emr-data-947775527574/airflow-finance/dags/assets/jars/spark-3.3.1/spark-snowflake_2.9.1/snowflake-jdbc-3.13.10.jar,s3://emr-data-947775527574/airflow-finance/dags/assets/jars/spark-3.3.1/spark-snowflake_2.9.1/spark-snowflake_2.12-2.9.3-spark_3.1.jar', <class 'dict'>)
[2023-08-13 12:00:34,261] {xcom.py:585} ERROR - Could not serialize the XCom value into JSON. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config.
[2023-08-13 12:00:34,263] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 2392, in xcom_push
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/xcom.py", line 197, in set
    map_index=map_index,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/xcom.py", line 582, in serialize_value
    return json.dumps(value).encode('UTF-8')
  File "/usr/local/lib/python3.7/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/usr/local/lib/python3.7/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/lib/python3.7/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/usr/local/lib/python3.7/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type type is not JSON serializable
[2023-08-13 12:00:34,295] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=submit_pyspark_script_to_emr, task_id=fetch_cluster_id_and_paramns, execution_date=20230813T120032, start_date=20230813T120033, end_date=20230813T120034
[2023-08-13 12:00:34,310] {standard_task_runner.py:97} ERROR - Failed to execute job 34 for task fetch_cluster_id_and_paramns (Object of type type is not JSON serializable; 176)
[2023-08-13 12:00:34,350] {local_task_job.py:156} INFO - Task exited with return code 1
[2023-08-13 12:00:34,402] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
