{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6888538c",
   "metadata": {},
   "source": [
    "### Preliminary Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39267dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dependencies (Packages To Be Downloaded From Maven)\n",
    "\n",
    "# 1. Required for Spark to interact with an Iceberg WH\n",
    "DEPENDENCIES = \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.1\"\n",
    "DEPENDENCIES += \",software.amazon.awssdk:bundle:2.20.18\"\n",
    "DEPENDENCIES += \",com.amazonaws:aws-java-sdk-bundle:1.11.901\"\n",
    "DEPENDENCIES += \",org.apache.hadoop:hadoop-aws:3.3.4\"\n",
    "\n",
    "# 2. Required only when catalog is stored in Postgres DB\n",
    "DEPENDENCIES += \",org.postgresql:postgresql:42.6.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523d3de",
   "metadata": {},
   "source": [
    "### 1. Local to MinIO - Using JDBC Catalog (PostGres DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526fa3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.sql.types import *\n",
    "\n",
    "def spark_local_to_minio(icb_catalog_name,\n",
    "                         iceberg_warehouse,\n",
    "                         storage_type,\n",
    "                         pg_user,\n",
    "                         pg_password,\n",
    "                         minio_bucket,\n",
    "                         minio_access_key,\n",
    "                         minio_secret_key,\n",
    "                         minio_end_point):\n",
    "\n",
    "    conf = (\n",
    "            SparkConf()\n",
    "            .setAppName('spark_local_to_minio')\n",
    "            #Dependencies\n",
    "            .set('spark.jars.packages', DEPENDENCIES)\n",
    "            #SQL Extensions\n",
    "            .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n",
    "            #Catalog Configuration\n",
    "            .set(f'spark.sql.catalog.{icb_catalog_name}', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "            .set(f'spark.sql.catalog.{icb_catalog_name}.catalog-impl', 'org.apache.iceberg.jdbc.JdbcCatalog')\n",
    "            .set(f'spark.sql.catalog.{icb_catalog_name}.uri', f'jdbc:postgresql://localhost:5439/{pg_db}')\n",
    "            .set(f'spark.sql.catalog.{icb_catalog_name}.jdbc.user', pg_user)\n",
    "            .set(f'spark.sql.catalog.{icb_catalog_name}.jdbc.password', pg_password)\n",
    "            .set(f'spark.sql.catalog.{icb_catalog_name}.jdbc.verifyServerCertificate', 'true')\n",
    "            .set(f'spark.sql.catalog.{icb_catalog_name}.jdbc.useSSL', 'true')\n",
    "            .set(f'spark.sql.defaultCatalog', icb_catalog_name)\n",
    "            .set(f'spark.sql.catalog.{icb_catalog_name}.warehouse', f's3a://{minio_bucket}/{iceberg_warehouse}/{storage_type}/')\n",
    "            # MinIO Configuration\n",
    "            .set('spark.hadoop.fs.s3a.access.key', minio_access_key)\n",
    "            .set('spark.hadoop.fs.s3a.secret.key', minio_secret_key)\n",
    "            .set(\"spark.hadoop.fs.s3a.endpoint\", minio_end_point)\n",
    "    )\n",
    "    \n",
    "    ## Start Spark Session\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "    print(\"Spark Session Running\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "############################################\n",
    "icb_catalog_name = 'pg_catalog'\n",
    "iceberg_warehouse = 'iceberg-warehouse-pg'\n",
    "storage_type = 'data-archives'\n",
    "\n",
    "pg_db = 'iceberg_warehouse_pg'\n",
    "pg_user = 'postgres'\n",
    "pg_password = 'postgres'\n",
    "\n",
    "minio_bucket = 'iceberg-bucket'\n",
    "minio_access_key = 'admin'\n",
    "minio_secret_key = 'password'\n",
    "minio_end_point = 'http://127.0.0.1:9000'\n",
    "\n",
    "spark = spark_local_to_minio(icb_catalog_name,\n",
    "                             iceberg_warehouse,\n",
    "                             storage_type,\n",
    "                             pg_user,\n",
    "                             pg_password,\n",
    "                             minio_bucket,\n",
    "                             minio_access_key,\n",
    "                             minio_secret_key,\n",
    "                             minio_end_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates A Test Table (Within MinIO) Into iceberg-warehouse-pg Warehouse\n",
    "# *NOTE*: Before this is run, you won't be able to see the ICB WH in the UI.\n",
    "\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE TABLE {icb_catalog_name}.TEST_SCHEMA.TEST_TABLE_MINIO_PG (\n",
    "             FIELD_1 BIGINT,\n",
    "             FIELD_2 varchar(50),\n",
    "             FIELD_3 DATE,\n",
    "             FIELD_4 DOUBLE,\n",
    "             FIELD_5 TIMESTAMP\n",
    "             )\n",
    "             USING iceberg\n",
    "             \"\"\")\n",
    "\n",
    "# Display Tables Created In The PG Catalog (TEST_SCHEMA)\n",
    "spark.sql(f'SHOW TABLES IN {icb_catalog_name}.TEST_SCHEMA').show(truncate=False)\n",
    "\n",
    "# select * from iceberg_warehouse_pg.public.iceberg_tables;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1076286",
   "metadata": {},
   "source": [
    "### 2. Local to S3 - Using Hadoop Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6604453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.sql.types import *\n",
    "\n",
    "def spark_local_to_s3(icb_catalog_name,\n",
    "                      iceberg_warehouse,\n",
    "                      storage_type,\n",
    "                      s3_bucket,\n",
    "                      s3_access_key,\n",
    "                      s3_secret_key):\n",
    "    \n",
    "    \n",
    "    os.environ.update({'AWS_ACCESS_KEY_ID': s3_access_key,\n",
    "                       'AWS_SECRET_ACCESS_KEY': s3_secret_key\n",
    "                   #   'AWS_SESSION_TOKEN': s3_session_token\n",
    "                      })\n",
    "    \n",
    "    conf = (\n",
    "            SparkConf()\n",
    "            .setAppName('spark_local_to_s3')\n",
    "            #packages\n",
    "            .set('spark.jars.packages', DEPENDENCIES)\n",
    "            #SQL Extensions\n",
    "            .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n",
    "            #Configuring Catalog\n",
    "            .set(f'spark.sql.catalog.{icb_catalog_name}', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "            .set(f'spark.sql.catalog.{icb_catalog_name}.type', 'hadoop')\n",
    "            .set(f'spark.sql.catalog.{icb_catalog_name}.warehouse', f's3a://{s3_bucket}/{iceberg_warehouse}/{storage_type}/')\n",
    "            .set(f'spark.sql.catalog.{icb_catalog_name}.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "    )\n",
    "\n",
    "    ## Start Spark Session\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    \n",
    "    print(\"Spark Session Running\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "############################################\n",
    "icb_catalog_name = 'hadoop_catalog'\n",
    "iceberg_warehouse = 'iceberg-warehouse-dev-hdp'\n",
    "storage_type = 'data-archives'\n",
    "\n",
    "s3_bucket = 'iceberg-bucket-9004'\n",
    "s3_access_key='XXXXX'\n",
    "s3_secret_key='XXXXX'\n",
    "\n",
    "spark = spark_local_to_s3(icb_catalog_name,\n",
    "                          iceberg_warehouse,\n",
    "                          storage_type,\n",
    "                          s3_bucket,\n",
    "                          s3_access_key,\n",
    "                          s3_secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as for method 1: unless this is run, IVB WH won't be displayed within the S3 Bucket\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE TABLE {icb_catalog_name}.TEST_SCHEMA.TEST_TABLE_EMR_S3_HDP (\n",
    "             FIELD_1 BIGINT,\n",
    "             FIELD_2 varchar(50),\n",
    "             FIELD_3 DATE,\n",
    "             FIELD_4 DOUBLE,\n",
    "             FIELD_5 TIMESTAMP\n",
    "             )\n",
    "             USING iceberg\n",
    "             \"\"\")\n",
    "\n",
    "spark.sql(f'SHOW TABLES IN {icb_catalog_name}.TEST_SCHEMA').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e05a60",
   "metadata": {},
   "source": [
    "### 3. Local to S3 - Deploy Spark App To EMR via CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b7704",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Set Up Credentials For aws_personal profile\n",
    "nano ~/.aws/credentials\n",
    "# structure\n",
    "# [aws_personal]\n",
    "# aws_access_key_id=xxxxxx\n",
    "# aws_secret_access_key=xxxxxx\n",
    "# region=xxxxxx\n",
    "\n",
    "# Confirm That EMR Cluster Exists & Waiting\n",
    "aws emr list-clusters --profile aws_personal\n",
    "\n",
    "# Add Step To AWS EMR Cluster Via CLI \n",
    "# Replace j-xxxxxxxxxxx with Cluster ID\n",
    "aws emr add-steps --profile aws_personal --cluster-id j-xxxxxxxxxxx \\ \n",
    "--steps '[{\n",
    "  \"Args\":[\"spark-submit\",\n",
    "    \"--deploy-mode\",\"client\",\n",
    "    \"--jars\",\"s3://iceberg-bucket-9004/jars/hadoop-aws-3.3.4.jar,s3://iceberg-bucket-9004/jars/bundle-2.20.18.jar,s3://iceberg-bucket-9004/jars/aws-java-sdk-bundle-1.11.901.jar,s3://iceberg-bucket-9004/jars/iceberg-spark-runtime-3.3_2.12-1.3.1.jar\",\n",
    "    \"--conf\",\"spark.sql.catalog.hadoop_catalog=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    \"--conf\",\"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"--conf\",\"spark.sql.catalog.hadoop_catalog.type=hadoop\",\n",
    "    \"--conf\",\"spark.sql.catalog.hadoop_catalog.warehouse=s3://iceberg-bucket-9004/iceberg-warehouse-emr/data-archives/\",\n",
    "    \"--conf\",\"spark.sql.catalog.hadoop_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "    \"s3://iceberg-bucket-9004/scripts/create_iceberg_wh_app.py\"],\n",
    "  \"Type\":\"CUSTOM_JAR\",\n",
    "  \"ActionOnFailure\":\"CONTINUE\",\n",
    "  \"Jar\":\"command-runner.jar\",\n",
    "  \"Properties\":\"\",\n",
    "  \"Name\":\"create_iceberg_wh_via_emr_cli\"\n",
    "}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53aaf0c",
   "metadata": {},
   "source": [
    "### 4. Prod to S3 - Deploy Spark App To EMR via Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04caf5f9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# The Spark Job, deployed to EMR via Airflow, will create an ICB WH, based on JSON Config Below\n",
    "{\n",
    "    \"local_conf\":{\n",
    "       \"local_sub_folder\":\"/assets/\",\n",
    "       \"files_to_upload\":[\"create_iceberg_wh_app.py\"]\n",
    "    },\n",
    "    \"s3_conf\":{\n",
    "       \"bucket_name\":\"iceberg-bucket-9004\",\n",
    "       \"s3_scripts_path\":\"scripts/\"\n",
    "    },\n",
    "    \"spark_submit_cmd\":{\n",
    "       \"cmd\":\"[\\\"spark-submit\\\", \\\"--deploy-mode\\\", \\\"client\\\"]\",\n",
    "       \"pyspark_exec\":\"scripts/create_iceberg_wh_app.py\"\n",
    "    },\n",
    "    \"spark_conf\":{\n",
    "         \"spark.sql.catalog.hadoop_catalog\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "         \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "         \"spark.sql.catalog.hadoop_catalog.type\": \"hadoop\",\n",
    "         \"spark.sql.catalog.hadoop_catalog.warehouse\": \"s3a://iceberg-bucket-9004/iceberg-warehouse-prod-hdp/data-archives/\",\n",
    "         \"spark.sql.catalog.hadoop_catalog.io-impl\": \"org.apache.iceberg.aws.s3.S3FileIO\"\n",
    "    },\n",
    "    \"spark_jars_conf\":{\n",
    "       \"bucket_prefix\":\"s3://iceberg-bucket-9004/\",\n",
    "       \"bucket_subfolder\":\"jars/\"\n",
    "    },\n",
    "    \"spark_jars_conf_value\":[\n",
    "         \"hadoop-aws-3.3.4.jar\",\n",
    "         \"bundle-2.20.18.jar\",\n",
    "         \"aws-java-sdk-bundle-1.11.901.jar\",\n",
    "         \"iceberg-spark-runtime-3.3_2.12-1.3.1.jar\"\n",
    "    ]\n",
    " }\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
